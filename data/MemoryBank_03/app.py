import gradio as gr
from assistant.memory import StableHuggingFaceEmbeddings
from assistant.state import UserState
from assistant.logic import (
    generate_conversational_response,
    answer_question_deep_thought,
    extract_and_store_facts,
    reinforce_memory,
    load_and_index_memory,
    reflect_and_update_memory
)
from config import MEMORY_FILE, EMBEDDING_MODEL_NAME

# --- 1. åˆå§‹åŒ–åº”ç”¨çº§çš„èµ„æº ---
embedding_model = StableHuggingFaceEmbeddings(model_name_or_path=EMBEDDING_MODEL_NAME)

# --- 2. å®šä¹‰Gradioçš„äº¤äº’å‡½æ•° (æœ€ç»ˆç‰ˆ) ---
def handle_chat_interaction(message: str, chat_history: list, user_state: UserState, deep_mode: bool):
    """(æœ€ç»ˆç‰ˆ)å¤„ç†èŠå¤©äº¤äº’ï¼Œä½¿ç”¨ç»Ÿä¸€çš„â€œæ ¸å¿ƒäººæ ¼â€æ¨¡å‹ã€‚"""
    if not user_state:
        chat_history.append({"role": "user", "content": message})
        chat_history.append({"role": "assistant", "content": "é”™è¯¯ï¼šè¯·å…ˆåœ¨ä¾§è¾¹æ è¾“å…¥æ‚¨çš„åå­—å¹¶åŠ è½½ã€‚"})
        return "", chat_history, user_state

    chat_history.append({"role": "user", "content": message})
    chat_history.append({"role": "assistant", "content": ""}) # å ä½
    yield "", chat_history, user_state

    final_answer = ""
    used_chunks = []

    # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šæ ¹æ®â€œæ·±åº¦æ€è€ƒâ€å¼€å…³ï¼Œé€‰æ‹©ä¸åŒçš„åº”ç­”æ¨¡å¼ ---
    if deep_mode:
        # æ·±åº¦æ¨¡å¼ï¼šè°ƒç”¨ä¸“é—¨çš„ã€å¤æ‚çš„åˆ†æå‡½æ•°
        print("[Gradio]: å·²æ¿€æ´»ã€æ·±åº¦æ€è€ƒæ¨¡å¼ã€‘")
        final_answer, used_chunks = answer_question_deep_thought(message, user_state)
    else:
        # æ ‡å‡†æ¨¡å¼ï¼šè°ƒç”¨å…¨æ–°çš„ã€ç»Ÿä¸€çš„â€œæ ¸å¿ƒäººæ ¼â€åº”ç­”å‡½æ•°
        print("[Gradio]: å·²æ¿€æ´»ã€æ ‡å‡†æ¨¡å¼ã€‘")
        final_answer, used_chunks = generate_conversational_response(message, user_state)

    # --- åå°è®°å¿†æ›´æ–° ---
    # 1. äº‹å®æå–ç°åœ¨å˜æˆä¸€ä¸ªé€šç”¨çš„åå°ä»»åŠ¡
    if extract_and_store_facts(user_state.user_name, MEMORY_FILE, message, final_answer):
        print("[Gradio]: å‘ç°æ–°äº‹å®ï¼Œæ­£åœ¨åå°é‡å»ºé•¿æœŸè®°å¿†ç´¢å¼•...")
        user_state.long_term_db = load_and_index_memory(MEMORY_FILE, user_state.user_name, user_state.embeddings, force_rebuild=True)

    # 2. è®°å¿†åŠ å›ºï¼ˆåªå¯¹ä½¿ç”¨äº†è®°å¿†çš„å›ç­”æœ‰æ•ˆï¼‰
    if used_chunks:
        reinforce_memory(user_state.user_name, used_chunks, MEMORY_FILE)
    
    # 3. æ¯æ—¥åæ€ï¼ˆåªåœ¨æœ‰è¶³å¤Ÿå¯¹è¯åè§¦å‘ï¼‰
    if reflect_and_update_memory(user_state.user_name, MEMORY_FILE):
         print("[Gradio]: åæ€æœºåˆ¶è§¦å‘ï¼Œæ­£åœ¨åå°é‡å»ºé•¿æœŸè®°å¿†ç´¢å¼•...")
         user_state.long_term_db = load_and_index_memory(MEMORY_FILE, user_state.user_name, user_state.embeddings, force_rebuild=True)

    # 4. æ›´æ–°çº¿æ€§å†å²å’ŒçŸ­æœŸç¼“å­˜
    user_state.conversation_history.append({"role": "user", "content": message})
    user_state.conversation_history.append({"role": "assistant", "content": final_answer})
    user_state.short_term_cache.add(message, final_answer)

    # ç”¨æœ€ç»ˆç­”æ¡ˆæ›´æ–°UI
    chat_history[-1]["content"] = final_answer
    yield "", chat_history, user_state


def load_user(user_name: str):
    if not user_name or not user_name.strip():
        error_message = [{"role": "assistant", "content": "è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„ç”¨æˆ·åã€‚"}]
        return None, gr.update(value=error_message), gr.update(interactive=False)
    state = UserState(user_name.strip(), embedding_model)
    initial_message = [{"role": "assistant", "content": f"ä½ å¥½ï¼Œ{user_name}ï¼æˆ‘å·²ç»å‡†å¤‡å¥½äº†ã€‚"}]
    return state, gr.update(value=initial_message), gr.update(interactive=True, placeholder=f"å‘{user_name}æé—®...")

def clear_session(user_state: UserState):
    if user_state:
        user_state.clear_memory_and_restart()
    return None, [], "è¯·å…ˆåœ¨å·¦ä¾§åŠ è½½ç”¨æˆ·...", gr.update(interactive=False)

# --- 3. æ„å»ºGradioç•Œé¢ ---
with gr.Blocks(theme=gr.themes.Soft(), title="å…¨èƒ½AIåŠ©æ‰‹ Demo") as demo:
    user_state = gr.State(None)
    gr.Markdown("# å…¨èƒ½AIåŠ©æ‰‹ Demo ğŸ¤–\nä¸€ä¸ªèåˆäº†åŒå±‚è®°å¿†ã€äº‹å®æå–å’Œå¤šæ¨¡å¼æ¨ç†çš„æ™ºèƒ½åŠ©ç†ã€‚")
    with gr.Row():
        with gr.Column(scale=1, min_width=250):
            gr.Markdown("### æ§åˆ¶é¢æ¿")
            user_name_input = gr.Textbox(label="æ‚¨çš„åå­—", placeholder="ä¾‹å¦‚ï¼šzhangsan", info="è¾“å…¥åå­—åæŒ‰å›è½¦åŠ è½½æ‚¨çš„ä¸“å±è®°å¿†ã€‚")
            deep_mode_toggle = gr.Checkbox(label="æ·±åº¦æ€è€ƒæ¨¡å¼", info="å›ç­”æ›´æ·±å…¥ï¼Œä½†å“åº”æ›´æ…¢ã€‚")
            clear_button = gr.Button("æ¸…é™¤è®°å¿†å¹¶é‡å¯", variant="stop")
        with gr.Column(scale=4):
            chatbot = gr.Chatbot(label="å¯¹è¯çª—å£", height=600, type="messages", avatar_images=("./user_avatar.png", "./assistant_avatar.png"))
            msg_input = gr.Textbox(label="è¾“å…¥æ¡†", placeholder="è¯·å…ˆåœ¨å·¦ä¾§åŠ è½½ç”¨æˆ·...", interactive=False, scale=4)
    
    user_name_input.submit(fn=load_user, inputs=[user_name_input], outputs=[user_state, chatbot, msg_input])
    msg_input.submit(fn=handle_chat_interaction, inputs=[msg_input, chatbot, user_state, deep_mode_toggle], outputs=[msg_input, chatbot, user_state])
    clear_button.click(fn=clear_session, inputs=[user_state], outputs=[user_state, chatbot, msg_input])

# --- 4. å¯åŠ¨åº”ç”¨ ---
if __name__ == "__main__":
    demo.launch()